{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Photos of Dogs and Cats (with 97% accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this tutorial, you will know:\n",
    "\n",
    "   * How to load and prepare photos of dogs and cats for modeling.\n",
    "   * How to develop a convolutional neural network for photo classification from scratch and improve model performance.\n",
    "   * How to develop a model for photo classification using transfer learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Overview\n",
    "\n",
    "This tutorial is divided into six parts; they are:\n",
    "\n",
    "   1. Dogs vs. Cats Prediction Problem\n",
    "   2. Dogs vs. Cats Dataset Preparation\n",
    "   3. Develop a Baseline CNN Model\n",
    "   4. Develop Model Improvements\n",
    "   5. Explore Transfer Learning\n",
    "   6. How to Finalize the Model and Make Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dogs vs. Cats Prediction Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dogs vs cats dataset refers to a dataset used for a Kaggle machine learning competition held in 2013.\n",
    "\n",
    "The dataset is comprised of photos of dogs and cats provided as a subset of photos from a much larger dataset of 3 million manually annotated photos. The dataset was developed as a partnership between Petfinder.com and Microsoft.\n",
    "\n",
    "The dataset was originally used as a CAPTCHA (or Completely Automated Public Turing test to tell Computers and Humans Apart), that is, a task that it is believed a human finds trivial, but cannot be solved by a machine, used on websites to distinguish between human users and bots. Specifically, the task was referred to as â€œAsirraâ€ or Animal Species Image Recognition for Restricting Access, a type of CAPTCHA. The task was described in the 2007 paper titled â€œAsirra: A CAPTCHA that Exploits Interest-Aligned Manual Image Categorizationâ€œ.\n",
    "\n",
    "    We present Asirra, a CAPTCHA that asks users to identify cats out of a set of 12 photographs of both cats and dogs. Asirra is easy for users; user studies indicate it can be solved by humans 99.6% of the time in under 30 seconds. Barring a major advance in machine vision, we expect computers will have no better than a 1/54,000 chance of solving it.\n",
    "\n",
    "â€” Asirra: A CAPTCHA that Exploits Interest-Aligned Manual Image Categorization, 2007.\n",
    "\n",
    "At the time that the competition was posted, the state-of-the-art result was achieved with an SVM and described in a 2007 paper with the title â€œMachine Learning Attacks Against the Asirra CAPTCHAâ€ (PDF) that achieved 80% classification accuracy. It was this paper that demonstrated that the task was no longer a suitable task for a CAPTCHA soon after the task was proposed.\n",
    "\n",
    "    â€¦ we describe a classifier which is 82.7% accurate in telling apart the images of cats and dogs used in Asirra. This classifier is a combination of support-vector machine classifiers trained on color and texture features extracted from images. [â€¦] Our results suggest caution against deploying Asirra without safeguards.\n",
    "\n",
    "â€” Machine Learning Attacks Against the Asirra CAPTCHA, 2007.\n",
    "\n",
    "The Kaggle competition provided 25,000 labeled photos: 12,500 dogs and the same number of cats. Predictions were then required on a test dataset of 12,500 unlabeled photographs. The competition was won by Pierre Sermanet (currently a research scientist at Google Brain) who achieved a classification accuracy of about 98.914% on a 70% subsample of the test dataset. His method was later described as part of the 2013 paper titled â€œOverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks.â€\n",
    "\n",
    "The dataset is straightforward to understand and small enough to fit into memory. As such, it has become a good â€œhello worldâ€ or â€œgetting startedâ€ computer vision dataset for beginners when getting started with convolutional neural networks.\n",
    "\n",
    "As such, it is routine to achieve approximately 80% accuracy with a manually designed convolutional neural network and 90%+ accuracy using transfer learning on this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dogs vs. Cats Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be downloaded for free from the Kaggle website, although I believe you must have a Kaggle account.\n",
    "\n",
    "If you do not have a Kaggle account, sign-up first.\n",
    "\n",
    "Download the dataset by visiting the Dogs vs. Cats Data page and click the â€œDownload Allâ€ button.\n",
    "\n",
    "This will download the 850-megabyte file â€œdogs-vs-cats.zipâ€ to your workstation.\n",
    "\n",
    "Unzip the file and you will see train.zip, train1.zip and a .csv file. Unzip the train.zip file, as we will be focusing only on this dataset.\n",
    "\n",
    "You will now have a folder called â€˜train/â€˜ that contains 25,000 .jpg files of dogs and cats. The photos are labeled by their filename, with the word â€œdogâ€ or â€œcatâ€œ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Dog and Cat Photos\n",
    "\n",
    " Looking at a few random photos in the directory, you can see that the photos are color and have different shapes and sizes.\n",
    "\n",
    " For example, letâ€™s load and plot the first nine photos of dogs in a single figure.\n",
    "\n",
    " The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dog photos from the dogs vs cats dataset\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.image import imread\n",
    "# define location of dataset\n",
    "folder = 'train/'\n",
    "# plot first few images\n",
    "for i in range(9):\n",
    "\t# define subplot\n",
    "\tpyplot.subplot(330 + 1 + i)\n",
    "\t# define filename\n",
    "\tfilename = folder + 'dog.' + str(i) + '.jpg'\n",
    "\t# load image pixels\n",
    "\timage = imread(filename)\n",
    "\t# plot raw pixel data\n",
    "\tpyplot.imshow(image)\n",
    "# show the figure\n",
    "pyplot.show()\n",
    "#Running the example creates a figure showing the first nine photos of dogs in the dataset.\n",
    "\n",
    "#We can see that some photos are landscape format, some are portrait format, and some are square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update the example and change it to plot cat photos instead; the complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cat photos from the dogs vs cats dataset\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.image import imread\n",
    "# define location of dataset\n",
    "folder = 'train/'\n",
    "# plot first few images\n",
    "for i in range(9):\n",
    "\t# define subplot\n",
    "\tpyplot.subplot(330 + 1 + i)\n",
    "\t# define filename\n",
    "\tfilename = folder + 'cat.' + str(i) + '.jpg'\n",
    "\t# load image pixels\n",
    "\timage = imread(filename)\n",
    "\t# plot raw pixel data\n",
    "\tpyplot.imshow(image)\n",
    "# show the figure\n",
    "pyplot.show()\n",
    "#Again, we can see that the photos are all different sizes.\n",
    "\n",
    "#We can also see a photo where the cat is barely visible (bottom left corner) and another that has two cats (lower right corner). This suggests that any classifier fit on this problem will have to be robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Standardized Photo Size\n",
    "\n",
    "The photos will have to be reshaped prior to modeling so that all images have the same shape. This is often a small square image.\n",
    "\n",
    "There are many ways to achieve this, although the most common is a simple resize operation that will stretch and deform the aspect ratio of each image and force it into the new shape.\n",
    "\n",
    "We could load all photos and look at the distribution of the photo widths and heights, then design a new photo size that best reflects what we are most likely to see in practice.\n",
    "\n",
    "Smaller inputs mean a model that is faster to train, and typically this concern dominates the choice of image size. In this case, we will follow this approach and choose a fixed size of 200Ã—200 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Photo Sizes (Optional)\n",
    "\n",
    "If we want to load all of the images into memory, we can estimate that it would require about 12 gigabytes of RAM.\n",
    "\n",
    "That is 25,000 images with 200x200x3 pixels each, or 3,000,000,000 32-bit pixel values.\n",
    "\n",
    "We could load all of the images, reshape them, and store them as a single NumPy array. This could fit into RAM on many modern machines, but not all, especially if you only have 8 gigabytes to work with.\n",
    "\n",
    "We can write custom code to load the images into memory and resize them as part of the loading process, then save them ready for modeling.\n",
    "\n",
    "The example below uses the Keras image processing API to load all 25,000 photos in the training dataset and reshapes them to 200Ã—200 square photos. The label is also determined for each photo based on the filenames. A tuple of photos and labels is then saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dogs vs cats dataset, reshape and save to a new file\n",
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "# define location of dataset\n",
    "folder = 'train/'\n",
    "photos, labels = list(), list()\n",
    "# enumerate files in the directory\n",
    "for file in listdir(folder):\n",
    "\t# determine class\n",
    "\toutput = 0.0\n",
    "\tif file.startswith('dog'):\n",
    "\t\toutput = 1.0\n",
    "\t# load image\n",
    "\tphoto = load_img(folder + file, target_size=(200, 200))\n",
    "\t# convert to numpy array\n",
    "\tphoto = img_to_array(photo)\n",
    "\t# store\n",
    "\tphotos.append(photo)\n",
    "\tlabels.append(output)\n",
    "# convert to a numpy arrays\n",
    "photos = asarray(photos)\n",
    "labels = asarray(labels)\n",
    "print(photos.shape, labels.shape)\n",
    "# save the reshaped photos\n",
    "save('dogs_vs_cats_photos.npy', photos)\n",
    "save('dogs_vs_cats_labels.npy', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example may take about one minute to load all of the images into memory and prints the shape of the loaded data to confirm it was loaded correctly.\n",
    "> ðŸ”°Note: running this example assumes you have more than 12 gigabytes of RAM. You can skip this example if you do not have sufficient RAM; it is only provided as a demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(25000, 200, 200, 3) (25000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the run, two files with the names â€˜dogs_vs_cats_photos.npyâ€˜ and â€˜dogs_vs_cats_labels.npyâ€˜ are created that contain all of the resized images and their associated class labels. The files are only about 12 gigabytes in size together and are significantly faster to load than the individual images.\n",
    "\n",
    "The prepared data can be loaded directly; for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and confirm the shape\n",
    "from numpy import load\n",
    "photos = load('dogs_vs_cats_photos.npy')\n",
    "labels = load('dogs_vs_cats_labels.npy')\n",
    "print(photos.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Photos into Standard Directories\n",
    "\n",
    "Alternately, we can load the images progressively using the Keras ImageDataGenerator class and flow_from_directory() API. This will be slower to execute but will run on more machines.\n",
    "\n",
    "This API prefers data to be divided into separate train/ and test/ directories, and under each directory to have a subdirectory for each class, e.g. a train/dog/ and a train/cat/ subdirectories and the same for test. Images are then organized under the subdirectories.\n",
    "\n",
    "We can write a script to create a copy of the dataset with this preferred structure. We will randomly select 25% of the images (or 6,250) to be used in a test dataset.\n",
    "\n",
    "First, we need to create the directory structure as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset_dogs_vs_cats                                          \n",
    "â”œâ”€â”€ test                                                      \n",
    "â”‚   â”œâ”€â”€ cats                                                                        \n",
    "â”‚   â””â”€â”€ dogs                                                                        \n",
    "â””â”€â”€ train                                                                           \n",
    "    â”œâ”€â”€ cats                                                                        \n",
    "    â””â”€â”€ dogs                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
